{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `TikaDocumentConverter`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run Apache Tika server\n",
    "  ```bash\n",
    "  docker run -d -p 127.0.0.1:9998:9998 apache/tika:latest\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On its own\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.converters import TikaDocumentConverter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = TikaDocumentConverter()\n",
    "res = converter.run(sources=[Path(\"./sample.pdf\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [Document(id=85e4d1e1926ac6bad093031bc9c1d56c3e5331ffaaa44eed39e994bc064cfb9f, content: '\n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  ThuyVT2\n",
       "  \n",
       "  \n",
       "  1\n",
       "  \n",
       "  ThuyVT2\n",
       "  \n",
       "  \n",
       "  \n",
       "  2\n",
       "  \n",
       "  VKS\n",
       "  VKS �VNGCloud Kubernetes Service) is...', meta: {'file_path': 'sample.pdf'})]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.converters import TikaDocumentConverter\n",
    "from haystack.components.preprocessors import DocumentCleaner\n",
    "from haystack.components.preprocessors import DocumentSplitter\n",
    "from haystack.components.writers import DocumentWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'writer': {'documents_written': 760}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_component(\"converter\", TikaDocumentConverter())\n",
    "pipeline.add_component(\"cleaner\", DocumentCleaner())\n",
    "pipeline.add_component(\"splitter\", DocumentSplitter(split_by=\"sentence\", split_length=5))\n",
    "pipeline.add_component(\"writer\", DocumentWriter(document_store=document_store))\n",
    "pipeline.connect(\"converter\", \"cleaner\")\n",
    "pipeline.connect(\"cleaner\", \"splitter\")\n",
    "pipeline.connect(\"splitter\", \"writer\")\n",
    "\n",
    "file_paths = [\"./sample.pdf\"]\n",
    "pipeline.run({\"converter\": {\"sources\": file_paths}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThuyVT2 1 ThuyVT2\f2 VKS\n",
      "VKS �VNGCloud Kubernetes Service) is a managed service on VNGCloud that help\n",
      " Kubernetes, an open-source platform developed by Google, is widely used to manage and deploy contai\n",
      "29) to ensure you can take advantage of the most advanced features. • Kubernetes Networking: VKS int\n",
      " • Integration of Native Blockstore �Container Storage Interface - CSI�� VKS allows you to manage Bl\n",
      "\f5 How VKS works? Below are the current concepts being provided to you by VKS� When you create a Pub\n",
      " At this time, these VMs �Nodes� cannot join the K8S cluster directly. In order for these VMs to joi\n",
      " Besides, using Private Node Group will help you control applications in the cluster more securely, \n",
      "..Private Cluster is the ideal choice for services that require strict access control, ensuring comp\n",
      " Comparison between using Public Cluster\n",
      "and Private Cluster\f7 other services in VNG Cloud. other se\n",
      " Scalability �AutoScaling) Easily scalable through Auto Scaling feature . Easily scalable through Au\n",
      " More complex, requires private and secure network configuration. Cost Usually lower because there i\n",
      " More flexible in applications that require security, but less flexible for applications that requir\n",
      "\f9 Announcements and Updates\f10 Release notes VKS �VNGCloud Kubernetes Service) has just released th\n",
      " This feature brings high flexibility to users who want to experience VKS. For more details, please \n",
      " Dec 5, 2024 Oct 23, 2024 https://docs-vngcloud-vn.translate.goog/vng-cloud-document/vn/vks/upgrade-\n",
      " This addition gives customers more options in deploying applications, especially useful for busines\n",
      " This allows you to configure each node group in the cluster to be located on different subnets with\n",
      "translate.goog/vng-cloud-document/vn/vks/network/cni\f12 the VKS cluster to communicate via internal \n",
      " VKS �VNGCloud Kubernetes Service) introduces the latest update to the existing VKS, bringing many n\n",
      ". Private Cluster is the ideal choice for services that require strict access control, ensuring comp\n",
      "translate.goog/vng-cloud-document/v/vn/vks/bat-dau-voi-vks/khoi-tao-mot-private-cluster\f13 Improve: \n",
      "27.12-vks.1724605200 ◦ Ubuntu-22.kube_v1.28.\n",
      "8-vks.1724605200 ◦ Ubuntu-22.kube_v1.29.1-vks.\n",
      "1724605200 Chú ý: • Để khởi tạo một Private Cluster, bạn cần chọn sử dụng một trong 3 image mới này.\n",
      " These events enhance your ability to monitor and manage your Kubernetes cluster. Aug 13, 2024 Aug 0\n",
      " status of Node through intuitive dashboards. To display data on the dashboard, users need to instal\n",
      " With the ability to adjust multiple parameters at the same time, managing a Kubernetes cluster beco\n",
      "goog/vng-cloud-document/v/vn/vks/giam-sat/metrics\f15 Improve: • Upgrade VNGCloud Controller Manager \n",
      " This improves network performance for applications running in the Private Node Group. • Number of n\n",
      " This helps save time and effort when moving Cluster from test resources to real resources. For more\n",
      " use. Timeout for Cluster creation is 1 hour and for Node Group is 3 hours . If after this time your\n",
      " This improvement helps users avoid configuration errors when using Terraform to automate Kubernetes\n",
      " VKS �VNGCloud Kubernetes Service) introduces the latest update to the existing VKS, bringing many n\n",
      " Attention: Because the old default Storage Class has been removed from the system, if you want to c\n",
      " Improve: • Upgrade VNGCloud Controller Manager Plugin: Add Annotation to configure Load Balancer to\n",
      " The default IAM Service Account is the IAM Service Account that is automatically created by the Jun\n",
      " This will be a way to help you monitor activities occurring with your Cluster, thereby limiting unu\n",
      " Currently, this time has been optimized by us to 02�30s to 03�00s depending on each Cluster and eac\n",
      " ◦ Change SSH mechanism from Port 22 to Port 234. If you encounter any problems with this official r\n",
      " Improve: • System optimization: Helps the system operate more smoothly and efficiently. • Bug Fixes\n",
      "27, 1.28, 1.29) to ensure you always take advantage of the most advanced features. • Kubernetes Netw\n",
      " April 17, 2024\f21 • Scaling & Healing Automatically: VKS automatically expands the Node group when \n",
      " With these breakthrough features, VKS promises to bring you a completely new Kubernetes management \n",
      "3. Using the latest version of kubectl helps avoid unforeseen problems. Step 1� Download the latest \n",
      "com/kubernetes-release/release/stable.txt ) For example, to download version v1.17.0 on Linux, run t\n",
      "com/kubernetes-release/release/`curl curl -LO https://storage.googleapis.com/kubernetes-release/rele\n",
      "/kubectl Necessary conditions Install kubectl on Linux Install kubectl binary with curl on Linux htt\n",
      "kubernetes.io/ kubernetes-xenial main\" | sudo tee -a /etc/apt/sources.list.d/kubernetes sudo snap in\n",
      " For example, to download v1.17.0 on macOS, run the command: $(curl -s https://storage.googleapis.co\n",
      "txt ) Step 2� Create kubectl binary executable via command: Step 3� Put the binary into your PATH en\n",
      "com/kubernetes-release/release/stable.txt)/bin\n",
      "\" curl -LO https://storage.googleapis.com/kubernetes-\n",
      " chmod +x ./kubectl sudo mv ./kubectl /usr/local/bin/kubectl kubectl version brew install kubernetes\n",
      "txt\f26 or command: Step 2� Check to make sure the version you installed is the latest: If you are on\n",
      "googleapis.com/kubernetes-release/release/v1.17. Install with Macports on macOS Install kubectl on W\n",
      "googleapis.com/kubernetes-release/release/stable.txt . Step 2� Include the binary in your PATH envir\n",
      " If you have previously installed Docker Desktop, you may need to set your PATH before the Docker De\n",
      "googleapis.com/kubernetes-release/release/stable.txt\f28 Note: Installation updates will be performed\n",
      "kube : Step 5� Move to the folder .kube you just created: Step 6� Configure kubectl to use a remote \n",
      " You can install kubectl from part of the Google Cloud SDK. Step 1� Install Google Cloud SDK . Step \n",
      " By default, kubectl's configuration is defined at ~/.kube/config . 2. Check kubectl is configured c\n",
      "com/sdk/\f30 1. If you see a response URL, kubectl is properly configured to access your cluster. 2. \n",
      " If kubectl cluster-info returns the url but you cannot access your cluster, then check if it is con\n",
      " kubectl cluster-info dump kubectl configuration options Enable shell autocompletion Bash on Linux I\n",
      " The above commands generate /usr/share/bash-completion/bash_completion , which is the main script o\n",
      " If successful, you are done setting up, otherwise add the following to ~/.bashrc your file: 1. Relo\n",
      "bashrc : • Add script to folder /etc/bash_completion.d : • If you have an alias for kubectl, you can\n",
      "com/scop/bash-completion#installation\f32 Note: bash-completion sources all completion scripts in /et\n",
      " This source script will enable the kubectl completion feature. However, the kubectl completion scri\n",
      "1�. Kubectl completion script does not work properly with bash-completion v1 and Bash 3.2. It is com\n",
      " Therefore, to use kubectl completion correctly on macOS, you must install Bash 4.1� ( instructions \n",
      " Note: As mentioned, these instructions assume you are using Bash 4.1�, which means that you will be\n",
      " If not, you can install it with Homebrew: Bash on macOS Introduce Install bash-completion https://g\n",
      " Now you must ensure that the kubectl completion script has been sourced in all your shell sessions.\n",
      " In this case, you don't need to do anything. Note: Installing the Homebrew way already sources all \n",
      "d/kubectl echo 'alias k=kubectl' >>~/.bashrcecho 'complete -F __start_kubectl k' Enable kubectl auto\n",
      " Source completion script in your shell will enable kubectl autocompletion. To make it work for all \n",
      " If you get the error complete:13: command not found: compdef, add the following line at the beginni\n",
      " Now these VMs �Nodes� can directly join the K8S cluster through this Public IP. By using Public Clu\n",
      " In order for these VMs to join the K8S cluster, you need to use a NAT Gateway ( NATGW ). NATGW acts\n",
      " Model\f36 Create a Public Cluster\n",
      "with Public Node Group To be able to initialize a Cluster and Depl\n",
      " Please refer here if you are not sure how to install and use kuberctl. In addition, you should not \n",
      "console.vngcloud.vn/overview Step 2� At the Overview screen , select Activate. Step 3� Wait until we\n",
      "translate.goog/vng-cloud-document/v/vn/vserver/compute-hcm03-1a/network/virtual-private-cloud-vpc\f37\n",
      " Please wait a few minutes for us to initialize your Cluster, the Cluster's status is now Creating .\n",
      "vn/k8s-cluster Step 2� The Cluster list is displayed, select the icon and select Download Config Fil\n",
      "console-dev.vngcloud.tech/overview\f38 The following is a guide for you to deploy the nginx service o\n",
      "19.1 ports: - containerPort: 80\n",
      "--apiVersion: v1\n",
      "kind: Service\n",
      "metadata: name: nginx-service\n",
      "spec: s\n",
      " kubectl get svc,deploy,pod -owide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) service/kubernetes Clust\n",
      "178.229 <none> 80/TCP NAME READY UP-TO-DATE AVAILABLE AGE CONTAI\n",
      "deployment.apps/nginx-app 1/1 1 1 7\n",
      "0.1 <none> 443/TCP service/nginx-app NodePort 10.96.215.192 <none> 30080:3\n",
      "service/nginx-service Clu\n",
      "96.178.229 <none> 80/TCP NAME READY UP-TO-DATE AVAILABLE AGE CONTA\n",
      "deployment.apps/nginx-app 1/1 1 1\n",
      " Specifically, access at https://hcm-3.console.vngcloud.vn/vserver/v-server/cloud-server . For examp\n",
      "28.231.65�31007/ If you want to expose this service through vLB Layer4, vLB Layer7, please refer to:\n",
      "vn/vserver/v-server/cloud-server\f42 Create a Public Cluster\n",
      "with Private Node Group To be able to in\n",
      " Please refer here if you are not sure how to install and use kuberctl. In addition, you should not \n",
      "translate.goog/vng-cloud-document/v/vn/vserver/compute-hcm03-1a/network/virtual-private-cloud-vpc\f43\n",
      "vngcloud.vn/vserver/network/route-table Step 2� In the navigation menu bar, select Network Tab/ Rout\n",
      " The input data length is between 5 and 50. It must not include leading or trailing spaces. Step 5� \n",
      " Step 6 : Select Create to create a new Route table. Initialize Route Table https://docs-vngcloud-vn\n",
      "0.0.0/0 • For Target, enter Target CIDR as the corresponding Palo Alto or Pfsense Network Interface \n",
      " To initialize a Cluster, follow the steps below: Step 1� Visit https://vks.console.vngcloud.vn/over\n",
      " After Activate successfully, select Create a Cluster Step 4� At the Cluster initialization screen, \n",
      " Please wait a few minutes for us to initialize your Cluster, the Cluster's status is now Creating .\n",
      "tech/overview\f45 After the Cluster is successfully initialized, you can connect and check the newly \n",
      " Step 3 : Rename this file to config and save it to the ~/.kube/config directory Step 4� Perform Clu\n",
      "vngcloud.tech/overview\f46 • Create nginx-service-lb4.yaml file with the following content: ◦ Deploy \n",
      "yaml Step 1 : Create Deployment for Nginx app.\f47 • Run the following command to test Deployment • I\n",
      "vngcloud.vn/vserver/load\n",
      "balancer/vlb/ For example, below I have successfully accessed the nginx app\n",
      "96.0.1 <none> 443/T\n",
      "service/nginx-app NodePort 10.96.215.\n",
      "192 <none> 3008\n",
      "service/nginx-service LoadBalancer 10.96.179.221 <pending> 80:3 NAME READY UP-TO-DAT\n",
      " Step 3: To access the just exported nginx app, you can\n",
      "use the URL with the format: https://hcm-3.c\n",
      "76.0.0/16 Subnet 10.76.0.\n",
      "4/24 Network Interface 1 10.76.0.3 Item Configuration Flavor 2�8 Volume 60GB VPC 10.76.\n",
      "0.0/16 Network Interface 1 10.76.255.4 Network Interface 2 10.\n",
      "76.0.4 Prerequisites Initialize Palo Alto\f50 Step 1� Visit https://marketplace.console.vngcloud.\n",
      "vn/ Step 2� At the main screen, search for Palo Alto , at Palo Alto services , select Launch . Step \n",
      " Step 4� Proceed to pay like normal resources on VNG Cloud. Step 1� After initializing Palo Alto fro\n",
      " To access the Palo Alto GUI you need a vServer running Windows. Then you access it using IP Interna\n",
      "vn/\f51 Step 3 : After logging in, you need to change your password for the first time. Please enter \n",
      "vngcloud.vn/vserver/network/route-table Step 2� In the navigation menu bar, select Network Tab/ Rout\n",
      "vn/vserver/network/route-table\f62 Step 3� Select Create Route table. Step 4� Enter a descriptive nam\n",
      " Step 5� Select VPC for your Route table. If you do not have a VPC, you need to create a new VPC acc\n",
      " Step 8� In the add new Route section , enter the following information: • For Destination, enter De\n",
      "8.8.8 or google.com Checking connection https://docs.vngcloud.\n",
      "vn/pages/viewpage.action?pageId=49648039\f63\f64 Pfsense as a NAT Gateway\n",
      "Use the instructions below t\n",
      " Step 3� Now, you need to configure Pfsense. Specifically, you can select the desired Volume, IOPS, \n",
      " Item Configuration Flavor 2�4 Volume 80 GB VPC 10.3.0.0/16 Network Interface 1 10.3.\n",
      "0.3 Prerequisites Initialize Pfsense https://marketplace.console.vngcloud.vn/\f65 Step 1� After initi\n",
      " Next, open the Any rule on the Security Group for the Pfsense server you just created. Opening the \n",
      "console.vngcloud.vn/vserver/v-server/cloud-server\f66 Step 3 : Open the rule on the firewall • Procee\n",
      " Specifically, follow these steps to create a Route table: Step 1� Visit https://hcm-3.console.vngcl\n",
      " Step 4� Enter a descriptive name for the Route table. Route table names can include letters (az, AZ\n",
      " If you do not have a VPC, you need to create a new VPC according to the instructions on the VPC Pag\n",
      "0.0.0/0 • For Target, enter Target CIDR as the Pfsense Network Interface 2 IP address. For example: \n",
      "vngcloud.vn/vserver/network/route-table\f79 Proceed to ping google.com or 8.8.8.\n",
      "8 to check • Before Enable NAT the server could not access the internet • After configuring NAT, pin\n",
      " The Private Cluster feature helps your K8S cluster to be as secure as possible, all connections are\n",
      " • Nodes : When created, Nodes in the Cluster will only have internal IPs and cannot go to the publi\n",
      "translate.goog/vng-cloud-document/v/vn/vks/bat-dau-voi-vks/khoi-tao-mot-public-cluster/khoi-tao-mot-\n",
      "..) ◦ Endpoint to connect to vServer service �Endpoint Name: vks-vserver\n",
      "endpoint-...\n",
      ") ◦ Endpoint to connect to vStorage service �Endpoint Name: vks-vstorage\n",
      "endpoint-...) You can view \n",
      " If you accidentally delete or edit these 4 endpoints, within a maximum of 5 minutes, the system wil\n",
      " When private clusters share a VPC, we will reuse them for these clusters. • Delete Private Service \n",
      " If you do not have a VPC or Subnet yet, please create a VPC and Subnet according to the instruction\n",
      " In addition, you should not use a kubectl version that is too old, we recommend that you use a kube\n",
      "vngcloud.vn Prerequisites Initialize Cluster https://docs-vngcloud-vn.translate.goog/vng-cloud-docum\n",
      " To initialize a Cluster, follow the steps below: Step 1� Visit https://vks.console.vngcloud.vn/over\n",
      " After Activate successfully, select Create a Cluster Step 4� At the Cluster initialization screen, \n",
      " https://vks.console-dev.vngcloud.tech/overview\f84 Warning: • A Cluster can have many Node Groups , \n",
      " After the Cluster is successfully initialized, you can connect and check the newly created Cluster \n",
      " Step 3 : Rename this file to config and save it to the ~/.kube/config folder Step 4� Because your C\n",
      "866230 23348 memcache.go:265] couldn't get current serve\n",
      "E0821 14:27:07.922272 23348 memcache.go:265\n",
      "go:265] couldn't get current serve\n",
      "E0821 14:27:12.055864 23348 memcache.go:265] couldn't get current\n",
      "9:6443: connectex: No co Connect and check the newly created\n",
      "Cluster information https://vks.console\n",
      " After SSH into the server, install kubectl according to the instructions here . • For example, I am\n",
      "kube/config kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\n",
      "kubernetes ClusterIP 10.96.\n",
      "vn/vng-cloud-document/v/vn/vserver/compute-hcm03-1a/server/ket-noi-vao-may-chu-ao/ket-noi-vao-may-ch\n",
      "vn/list • Perform Repository and Repository initialization according to instructions here . For exam\n",
      "com/engine/install/\f87 Warning: • If you want to create a Private Reposity, to pull an image from th\n",
      "vn -u <repository_user> docker login vcr.vngcloud.vn -u 53461-user_demo docker tag SOURCE_IMAGE[:TAG\n",
      "vngcloud.vn/53461-repo_demo/nginx-demo:latest docker push vcr.vngcloud.vn/REPO_NAME/IMAGE[:TAG] dock\n",
      "vn/53461-repo_demo/nginx-demo:latest vi nginx.yaml Deploy a Workload\f89 Then, enter the content for \n",
      "yaml\f90 Step 2� Check Deployment and Service information before exposing it to the Internet. • Run t\n",
      "0.1 <none> 44\n",
      "service/nginx-service LoadBalancer 10.96.81.236 116.\n",
      "118.88.236 80 NAME READY UP-TO-DATE AVAILABLE AGE CONTA\n",
      "deployment.apps/nginx-app 1/1 1 1 3m32s ngin\n",
      "vngcloud.vn/vserver/load-balancer/vlb\f91 Step 3� To access the just exported nginx app, you can use \n",
      "vn/vserver/load\n",
      "balancer/vlb/ For example, below I have successfully accessed the nginx app with the\n",
      " • To ensure the private cluster works effectively, we have automatically added the Subnet you choos\n",
      "vngcloud.vn/vserver/load-balancer/vlb/detail/lb-927c0b5f-5bcf-4ee1-b645-41d6a0caeecb\f92 Expose a ser\n",
      " • Installed and configured kubectl on your device. Please refer here if you are not sure how to ins\n",
      " To initialize a Cluster, follow the steps below: Step 1� Visit https://vks.console.vngcloud.vn/over\n",
      " After Activate successfully, select Create a Cluster Step 4� At the Cluster initialization screen, \n",
      " Step 5� Select Create Kubernetes cluster. Please wait a few minutes for us to initialize your Clust\n",
      "vngcloud.vn/k8s-cluster Step 2� The Cluster list is displayed, select the iconand select Download Co\n",
      " kubectl get nodes Connect and check the newly created\n",
      "Cluster information https://vks.console-dev.v\n",
      " If you have enabled the Enable vLB Native Integration Driver option , then we have pre-installed th\n",
      "vngcloud.vn/iam/service-accounts\f95 vServerFullAccess created by VNG Cloud, you cannot delete these \n",
      " Refer to https://helm.sh/docs/intro/install/ for instructions on how to install. • Add this repo to\n",
      "global.clientID= <Lấy ClientID của Service A --set cloudConfig.global.clientSecret= <Lấy ClientSecre\n",
      "sh/docs/intro/install/\f96 For example, in the image below you have successfully installed vngcloud-c\n",
      "1 ports: - containerPort: 80\n",
      "--apiVersion: v1\n",
      "kind: Service\n",
      "metadata: name: nginx-service\n",
      "spec: sele\n",
      "vngcloud.vn/vserver/load\n",
      "balancer/vlb/ For example, below I have successfully accessed the nginx app\n",
      "96.0.1 <none> 443/T\n",
      "service/nginx-service LoadBalancer 10.96.74.\n",
      "154 <pending> 80:31 NAME READY UP-TO-DATE AVAILABLE AGE CONTAI\n",
      "deployment.apps/nginx-app 0/1 1 0 2s \n",
      " Attention: • Changing the name or size �Rename, Resize) of the Load Balancer resource on vServer Po\n",
      "goog/vng-cloud-document/v/vn/vks/network/lam-viec-voi-network-load-balancing-nlb\f100 Expose a servic\n",
      " Please refer here if you are not sure how to install and use kuberctl. In addition, you should not \n",
      "console.vngcloud.vn/overview Step 2� At the Overview screen , select Activate. Step 3� Wait until we\n",
      " You can keep these default values or Prerequisites Initialize Cluster https://docs-vngcloud-vn.tran\n",
      " Please wait a few minutes for us to initialize your Cluster, the Cluster's status is now Creating .\n",
      "vn/k8s-cluster Step 2� The Cluster list is displayed, select the iconand select Download Config File\n",
      "console-dev.vngcloud.tech/overview\f102 Attention: When you initialize the Cluster according to the i\n",
      " Workload. Initialize Service Account • Create or use a service account created on IAM and attach po\n",
      "vn/iam/service-accounts\f103 vServerFullAccess created by VNG Cloud, you cannot delete these policies\n",
      "sh/docs/intro/install/ for instructions on how to install. • Add this repo to your cluster via the c\n",
      "global.clientID= <Lấy ClientID của Service A --set cloudConfig.global.clientSecret= <Lấy ClientSecre\n",
      "sh/docs/intro/install/\f104 • Create nginx-service-lb7.yaml file with the following content: • Deploy\n",
      "\f105 • Run the following command to test Deployment • If the results are returned as below, it means\n",
      "1 <none> 443/TCP service/nginx-service NodePort 10.96.25.133 <none> 80:32572 NAME READY UP-TO-DATE A\n",
      "k8s.io/v1\n",
      "kind: Ingress\n",
      "metadata: name: nginx-ingress\n",
      "spec: ingressClassName: \"vngcloud\" defaultBack\n",
      "key, which are the certificate and private key to use for TLS. If you want to use a Certificate for \n",
      "181.129/ apiVersion: networking.k8s.io/v1\n",
      "kind: Ingress\n",
      "metadata: name: example-ingress annotations:\n",
      "class: \"vngcloud\" # this annotation is deprec vks.vngcloud.vn/load-balancer-id: \"lb-6cdea8fd-4589-41\n",
      "example.com rules: - host: host.example.com http: paths: - path: /path1 pathType: Exact backend: ser\n",
      "93.181.129/\f109 You can see more about ALB at Working with Application Load Balancer �ALB �. Attenti\n",
      " To prevent this problem, use kubectl Cluster resource management. https://docs-vngcloud-vn.translat\n",
      " Exactly. Below are our specific instructions to help you implement this usecase. • You have initial\n",
      " If your appversion is lower than this standard version, you can perform the upgrade according to th\n",
      " The input value is a list of service names in Load Balancer using Proxy Protocol. • Finally, please\n",
      "0/0 kubectl annotate service -n kube-system nginx-ingress-controller-controll vks.vngcloud.vn/enable\n",
      "75 to curl to host kkk.example.com as follows: • The recorded log result has this Client IP informat\n",
      "example.com http: paths: - backend: service: name: prometheus-node-exporter port: number: 9100 path:\n",
      " • Installed and configured kubectl on your device. Please refer here if you are not sure how to ins\n",
      " To initialize a Cluster, follow the steps below: Step 1� Visit https://vks.console.vngcloud.vn/over\n",
      " After Activate successfully, select Create a Cluster Prerequisites Initialize Cluster https://docs-\n",
      " Step 5� Select Create Kubernetes cluster. Please wait a few minutes for us to initialize your Clust\n",
      "vngcloud.vn/k8s-cluster Step 2� The Cluster list is displayed, select Download Config File to downlo\n",
      " kubectl get nodes Connect and check the newly created\n",
      "Cluster information https://vks.console-dev.v\n",
      " If you have enabled the Enable BlockStore Persistent Disk CSI Driver option , we have pre-installed\n",
      " Initialize Service Account • Create or use a service account created on IAM and attach policy: vSer\n",
      " ◦ After successful creation, you need to save the Client_ID and Secret_Key of the Service Account t\n",
      " • Add this repo to your cluster via the command: • Replace your K8S cluster's ClientID, Client Secr\n",
      "vksClusterId=${VNGCLOUD_VKS_CLUSTER kubectl get pods -n kube-system | grep vngcloud-ingress-controll\n",
      "1 ports: - containerPort: 80\n",
      "--apiVersion: v1\n",
      "kind: Service\n",
      "metadata: name: nginx-service\n",
      "spec: sele\n",
      "96.0.1 <none> 443/T\n",
      "service/nginx-app NodePort 10.96.215.\n",
      "192 <none> 3008\n",
      "service/nginx-service LoadBalancer 10.96.179.221 <pending> 80:3 NAME READY UP-TO-DAT\n",
      "1 Step 2: Check the Deployment and Service information\n",
      "just deployed Create Persistent Volume\f119 • \n",
      "vn # The VNG-CLOUD CSI parameters: type: vtype-61c3fc5b-f4e9-45b4-8957-8aa7b6029018 # The volume typ\n",
      " Follow these steps to enable the Snapshot service: Step 1� Visit https://hcm-3.console.vngcloud.vn/\n",
      "console.vngcloud.vn/vserver/block-store/snapshot/overview\f121 Step 2� Select Activate Snapshot Servi\n",
      " Refer to https://helm.sh/docs/intro/install/ for instructions on how to install. • Add this repo to\n",
      "sh/docs/intro/install/\f122 For example, in the image below you have successfully installed vngcloud\n",
      "\n",
      "vngcloud.vn\n",
      "deletionPolicy: Delete\n",
      "parameters: force-create: \"false\"\n",
      "--\n",
      "apiVersion: snapshot.storage\n",
      "yaml kubectl get sc,pvc,pod -owide Create a snapshot.yaml file with the following content: Check the\n",
      " For example, below I am changing the Persistent Volume IOPS from 200 �Volume type id = vtype-61c3fc\n",
      "vngclou\n",
      "storageclass.storage.k8s.io/sc-iops-200-retain (default) bs.csi.\n",
      "vngclou NAME STATUS VOLUME persistentvolumeclaim/my-expansion-pvc Bound pvc-14456f4a-ee9e-435d\n",
      "NAME \n",
      "vn/volume-type: \"volume-type-id\" . Now, edit this annotation to the Volume type id with the IOPS you\n",
      "vn/volume-type: \"vtype-85b39362-a360-4bbb-9afa-a36a40 kubectl.kubernetes.io/last-applied-configurati\n",
      "kubernetes.io/bound-by-controller: \"yes\" volume.beta.kubernetes.io/storage-provisioner: bs.\n",
      "csi.vngcloud.vn volume.kubernetes.io/storage-provisioner: bs.\n",
      "csi.vngcloud.vn creationTimestamp: \"2024-04-21T14:16:53Z\" finalizers: - kubernetes.io/pvc-protection\n",
      "yaml with the following content: kubectl patch pvc my-expansion-pvc -p '{\"spec\":{\"resources\":{\"reque\n",
      "g. 1.24 to 1.25� • Upgrade to a newer Patch Version (for example: 1.24.\n",
      "2�VKS.100 to 1.24.5� VKS.200� To upgrade the Control Plane version, you can follow these instruction\n",
      "console.vngcloud.vn/overview Step 2� At the Overview screen , select the Kubernetes Cluster menu. St\n",
      " The new version needs to be valid and compatible with the current version of the cluster. Specifica\n",
      "25� • Upgrade to a newer Patch Version (for example: 1.24.2�VKS.100 to 1.24.\n",
      "5� VKS.200� Step 4� The VKS system will upgrade the Control Plane components of the Cluster to the n\n",
      " Besides, the VKS system automatically upgrades https://vks.console-dev.vngcloud.tech/overview\f127 C\n",
      " • Below are a few notes before, during and after the upgrade process, please refer to: Before getti\n",
      " While doing: • Monitor cluster status: Monitor cluster status during the upgrade process. The clust\n",
      " Note: • Upgrading Control Plane Version may take some time depending on the size and complexity of \n",
      "25 (current Control Plane Version), but cannot upgrade to other versions. To perform a Node Group Ve\n",
      " Select a Cluster where you want to upgrade Node Group Version . Step 3� Select the icon and select \n",
      "24 to 1.25� Step 5� The VKS system will upgrade all Node Groups to the Control Plane version. After \n",
      " Besides, the VKS system automatically https://vks.console-dev.vngcloud.tech/overview\f130 upgrades t\n",
      " • Below are a few notes before, during and after the upgrade process, please refer to: Before getti\n",
      " Node Group status will change to UPDATING and after completion will return to ACTIVE. • Check syste\n",
      " • In some rare cases, upgrading Node Group Version may fail. If this happens, the VKS system will a\n",
      " Access the IAM Portal here , create a Service Account with VKS Full Access authority . Specifically\n",
      " Access the VKS Portal here , Activate the VKS service on the Overview tab. Please wait until we suc\n",
      "vngcloud.vn/\f132 • Download and install Terraform for your operating system from https://developer.h\n",
      " Initialize Terraform configuration: • Create a file variable.tf and declare Service Account informa\n",
      "tf: you need to replace the Client ID and Client Secret created in step 1 in this file. • The file m\n",
      "tf file, to initialize a cluster with a node group, you must pass in the following parameters: varia\n",
      "2\" } }\n",
      "} provider \"vngcloud\" { token_url = \"https://iamapis.vngcloud.vn/accounts-api/v2/auth/to clie\n",
      "api.vngcloud.vn/vserver/vserver-gatew vlb_base_url = \"https://hcm-3.api.vngcloud.\n",
      "vn/vserver/vlb-gateway\"\n",
      "} resource \"vngcloud_vks_cluster\" \"primary\" { name = \"my-cluster\" descriptio\n",
      "0/16\" enable_private_cluster = false network_type = \"CALICO\" vpc_id = \"net-xxxxxxxx-xxxx-xxxxx-xxxx-\n",
      " 1. Apply Terraform configuration: • Run command terraform apply. This command will create a Kuberne\n",
      " e ab e_p a e_ odes a se ssh_key_id= \"ssh-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\" labels = { \"mylabel\n",
      " ◦ Install the NVIDIA GPU Operator in a VKS cluster. ◦ Deploy your GPU workload in a VKS cluster. ◦ \n",
      " • A VKS cluster with at least one NVIDIA GPU nodegroup. • kubectl command-line tool installed on yo\n",
      " • �Optional) Other tools and libraries that you can use to monitor and manage your Kubernetes resou\n",
      "com/gpu label, which can be used to filter the nodes that have GPUs. The nvidia.com/gpu label is use\n",
      "0 \\ -n gpu-operator --create-namespace \\ oci://vcr.vngcloud.vn/81-vks-public/vks-helm-charts/gpu-ope\n",
      "nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html\f138 Operator will only d\n",
      "com/gpu label, which means that the node has GPUs. • These labels also tell that this node is using \n",
      " The program is provided as a container image that you can deploy in your VKS cluster. See file cuda\n",
      "metadata.labels' | grep \"nvidi POD_NAME=$(kubectl -n gpu-operator get pods -l app=nvidia-device-plug\n",
      " The purpose of this Deployment is to create and manage a single pod running a TensorFlow container \n",
      "githubusercontent.com/vngcloud/kubernetes-sample-apps/ma # Check the pods\n",
      "kubectl get pods # Check p\n",
      " The following tables summarizes the difference between the GPU sharing modes supported by NVIDIA GP\n",
      " • GPU time\n",
      "sharin g is optim al for scena rios to avoid idling costly GPUs wher e full isolati on a\n",
      " For exam ple, MPI jobs with inter\n",
      "MPI rank parall elism. With these jobs, each small CUDA proce ss \n",
      " fully satur ate the whole GPU. • Workl oads that use CUDA MPS need to tolera te the mem ory prote c\n",
      " The replicas field specifies the number of pods that can share the GPU. The replicas field should b\n",
      " • This configuration will apply to all nodes in the cluster that have the nvidia.com/gpu label. To \n",
      "com/vngcloud/kubernetes-sample-apps/ma # Patch the ClusterPolicy\n",
      "kubectl patch clusterpolicies.nvidi\n",
      "com/gpu label. • The configuration is considered successful if the ClusterPolicy\n",
      "STATUS is ready . •\n",
      "replicas is set to 4, you can deploy up to 4 pods that share the GPU. • My cluster has only 1 GPU no\n",
      " • VKS uses NVIDIA's Multi-Process Service �MPS�. NVIDIA MPS is an alternative, binary-compatible im\n",
      "com/vngcloud/kubernetes-sample-apps/main/nvidia-gpu/manifest/time-slicing-verification.yaml\f149 sing\n",
      "com/gpu # Only apply for the node with the replicas: 4 # Allow 4 pods to share the GPU # Delete the \n",
      "nvidia.com/cluster-policy \\ -n gpu-operator --type merge \\ -p '{\"spec\": {\"dcgmExporter\": {\"enabled\":\n",
      "com/gpu label. • The configuration is considered successful if the ClusterPolicy\n",
      "STATUS is ready . •\n",
      "replicas is set to 4, you can deploy up to 4 pods that share the GPU. • Until now, we have configure\n",
      " • In this guideline, I add a new RTX�4090 into the cluster. • This configuration should be greate i\n",
      "githubusercontent.com/vngcloud/kubernetes-sample-apps/ma # Check the pods\n",
      "kubectl get pods # Check t\n",
      " For example: ◦ NodeGroup 1 includes the instance of GPU RTX 2080Ti with 4 pods sharing the GPU usin\n",
      "com/gpu replicas: 8 # Allow the node using Configure Multiple Node-Specific Configurations\f152 • Now\n",
      "githubusercontent.com/vngcloud/kubernetes-sample-apps/ma # Patch the ClusterPolicy\n",
      "kubectl patch clu\n",
      "com/cluster-policy \\ -n gpu-operator --type merge \\ -p '{\"spec\": {\"dcgmExporter\": {\"enabled\": false}\n",
      "com/vngcloud/kubernetes-sample-apps/raw/main/nvidia-gpu/manifest/tensorflow-mnist-sample.yaml\f153 • \n",
      " Execute the following command to install the Prometheus Stack and Prometheus Adapter in your VKS cl\n",
      "0.2 \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmVal # Install and configure P\n",
      "vn/81-vks-public/vks-helm-charts/prometheus-adapt --version 4.10.0 \\ --set prometheus.url=http://${p\n",
      "svc.clu # Check the resources of Prometheus are running\n",
      "kubectl -n prometheus get all # Enable the D\n",
      "k8s.io/v1beta1 | jq -r . | grep # Forward the Prometheus Adapter to your local machine\n",
      "kubectl -n pr\n",
      " DCGM_FI_DEV_M\n",
      "EM_COPY_UTIL Gauge Percentage Memory usage. DCGM_FI_DEV_E\n",
      "NC_UTIL Gauge Percentage En\n",
      " DCGM_FI_DEV_F\n",
      "B_USED Gauge MB Number of used frame buffers. The value is the same as the value of m\n",
      "com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html#\f156 • To enable this feature, you MUST:\n",
      " • If you already installed Keda in your cluster, you can use the ScaledObject to scale the GPU Node\n",
      " Default: 0 maxReplicaCount: 3 # Optional. Default: 100 triggers: # Will be trigger if either of the\n",
      "5' # Scale the GPU Nodegroup when the GPU memor Autoscaling GPU Resources https://keda.sh/\f157 • The\n",
      " • Now let's install Keda in your cluster by executing the below command: • Apply scaling-app.yaml m\n",
      " This manifest will scale the GPU Nodegroup based on the GPU usage. • When the ScaledObject Ready va\n",
      "14.2 kubectl -n keda get all kubectl apply -f \\ https://github.com/vngcloud/kubernetes-sample-apps/r\n",
      "yaml\f158 Clusters\n",
      "A cluster in Kubernetes is a collection of one or more virtual machines �VMs� conn\n",
      " If you do not have any SSH key, please create an SSH key according to the instructions here . • Ins\n",
      "console.vngcloud.vn/overview Step 2� At the Overview screen , select Activate. Step 3� Wait until we\n",
      " Step 4� At the Cluster initialization screen, we have set up information for the Cluster and a Defa\n",
      " The name can only contain alphanumeric characters (az, AZ, 0�9, '_', '-'). Your input data length m\n",
      " ▪ Description: Enter the information you want to note for the Cluster to create a separate mark for\n",
      "255.0.0 / 172.16.0.\n",
      "0 � 172.24.0.0 / 192.168.\n",
      "0.0� ▪ VPC� Select an existing VPC that meets K8S requirements to create your Cluster. Before choosi\n",
      " For more information, see Create a VPC . ▪ Subnet: By default, all available subnets in the VPC spe\n",
      "com/vngcloud/docs/blob/main/English/vserver/compute-hcm03-1a/network/virtual-private-cloud-vpc.md\f16\n",
      " • Minimum node : minimum number of nodes that the Cluster needs to have. • Maximum node : maximum n\n",
      " Default Max surge = 1 - upgrade only one node at a time. with maxUnavailable • Max unavailable : li\n",
      " ◦ Node Group Volume Setting: Boot Volume Configuration – Parameters are set by default by the syste\n",
      " ◦ Node Group Metadata Setting: You can enter the corresponding Metadata for the Node Group. • Plugi\n",
      " Step 6� When the Cluster status is Active , you can view Cluster information and Node Group informa\n",
      " Step 3� In the successfully created Cluster , select the icon and select Download Config File. Step\n",
      "tech/overview\f162 When you no longer need to use a Kubernetes Cluster, you should delete the resourc\n",
      " • ETCD. The system may not delete the following resources: • The Load Balancer is integrated into t\n",
      "vngcloud.vn/overview Step 2� At the Overview screen , select the Kubernetes Cluster menu. Step 3� In\n",
      "vngcloud.tech/overview\f163 Public Cluster and Private Cluster\n",
      "Below are the current concepts being p\n",
      " When you create a Public Cluster with a Private Node Group , the VKS system will: • Create VM witho\n",
      " Pfsense will help you manage incoming and outgoing network traffic (inbound and outbound traffic) e\n",
      " Other services in VNG Cloud such as: vStorage, vCR, vMonitor, VNGCloud APIs,...Private Cluster is t\n",
      " Private Cluster 3. Comparison between using Public Cluster\n",
      "and Private Cluster\f165 other services i\n",
      " Access management More difficult to control, access can be managed through the Whitelist feature St\n",
      " Configuration and deployment Simpler because it does not require setting up an internal network. Mo\n",
      " Flexibility High, easy to change and access services. More flexible in applications that require se\n",
      " Provides stable and secure connectivity, but requires more complex configuration and management, as\n",
      "25� • Upgrade to a newer Patch Version (for example: 1.24.2�VKS.100 to 1.24.\n",
      "5� VKS.200� To upgrade the Control Plane version, you can follow these instructions: Step 1� Visit h\n",
      " Step 3� Select the icon and select Upgrade control plane version to upgrade the control plane versi\n",
      " 1.24 to 1.25� • Upgrade to a newer Patch Version (for example: 1.24.2�VKS.\n",
      "100 to 1.24.5� VKS.200� Step 4� The VKS system will upgrade the Control Plane components of the Clus\n",
      " Attention: • Upgrading Control Plane Version is optional and independent of upgrading Node Group Ve\n",
      "tech/overview\f168 Control Plane Version when the current K8S Version used for your Cluster exceeds t\n",
      " • Ensure cluster availability: Cluster must be in active state �ACTIVE) and all nodes must be HEALT\n",
      " After implementation: • Check cluster availability: Confirm that the cluster has been upgraded succ\n",
      "\f170 Whitelist The IP Whitelist feature on VKS's Private Node Group mode allows you to only allow sp\n",
      " Recommendations for Using Whitelist in Cluster Models: • Recommendation : Not recommended to use wh\n",
      " To use the IP Whitelist feature on Private Node Group mode, you need to perform the following steps\n",
      "251.0/24\n",
      "116.118.95.0/24\n",
      "58.\n",
      "84.1.0/24\n",
      "58.84.2.\n",
      "0/24\n",
      "61.28.226.0/24\n",
      "61.28.\n",
      "227.0/24\n",
      "61.28.229.0/24\n",
      "61.\n",
      "28.230.0/24\n",
      "61.28.231.\n",
      "0/24\n",
      "180.93.182.0/24\n",
      "61.28.\n",
      "233.0/24\n",
      "61.28.235.0/24\n",
      "61.\n",
      "28.236.0/24\n",
      "61.28.238.\n",
      "0/24\n",
      "180.93.183.0/24 2. Public Cluster Includes Private Node\n",
      "Group Going Through NAT Gateway\n",
      "(Pfsens\n",
      " Private Cluster Includes Public Node\n",
      "Group or Private Node Group Edit Whitelist\f172 Step 1� Visit h\n",
      " Step 4� Now, the Edit Whitelist screen displays, you can enter the IP address you want to allow acc\n",
      "console-dev.vngcloud.tech/overview\f173 Stop POC\n",
      "Before learning how to Stop POC for your resources o\n",
      " • Perform Stop POC to renew the POC Cluster into a normal Cluster. To continue using the resource t\n",
      "vngcloud.vn/vng-cloud-document/v/vn/quan-ly-hoa-don-chi-phi-and-tai-nguyen-tren-vng-cloud/trai-nghie\n",
      " Please press Stop to turn off the POC option for your Cluster. Attention: • After stopping POC on V\n",
      "c\f177 Node Groups\n",
      "Node Group is an important concept in Kubernetes, used to manage groups of nodes �\n",
      " Step 3� At the Node Group initialization screen, we have set up information for your Node Group. Yo\n",
      " ◦ Auto Scaling: Enable auto-scaling in your Cluster. Auto scaling helps automatically adjust the nu\n",
      "vngcloud.tech/overview\f178 ▪ Maximum node : maximum number of nodes that the Cluster can scale to. ◦\n",
      " Default Max surge = 1 - upgrade only one node at a time. with maxUnavailable ▪ Max unavailable : li\n",
      " • Node Group Volume Setting: Boot Volume Configuration – Parameters are set by default by the syste\n",
      " The status of the Node Group is currently Creating . Step 6� When the Node Group status is Active ,\n",
      "\f179 For Node Group, you can edit the parameters: Number of Nodes, Auto Scaling, Upgrade Strategy, S\n",
      " Step 3� On the screen containing the list of existing Node Groups, in the Node Group you want to ed\n",
      " • Edit Upgrade Strategy feature : you can change ◦ Node Group upgrade strategy: Node Group upgrade \n",
      "tech/overview\f180 the same time). Default Max surge = 1 - upgrade only one node at a time. with maxU\n",
      " Attention: When you no longer need to use the Node Group, delete them to save costs. When deleting \n",
      " Step 3� In the successfully created Cluster , select the Node Group you want to delete and select D\n",
      "tech/overview\f181 Auto Healing On the VKS system, the Auto Healing feature is applied to each Node G\n",
      " Restarting the node can help fix temporary errors and return the node to normal operation. • Minimi\n",
      " This process is performed in 2 steps: • Step 1� The VKS system drains the node, which means moving \n",
      " To avoid the error above, you need to: ◦ Make sure you have enough credits: If you're a prepaid use\n",
      "console.vngcloud.vn/vserver/limit\f183 Auto Scaling Auto Scaling for Cluster is a feature in Kubernet\n",
      " When workloads are higher, the cluster automatically creates additional nodes to ensure application\n",
      " Ensure availability: Auto Scaling helps ensure that the cluster is available to meet demand and avo\n",
      " Overview Mechanism of action Scale up mechanism: the Procuracy system performs\n",
      "scale up when\f184 • \n",
      "\f185 Attention: • When the system performs Auto Scaling, creating a new node may encounter an error \n",
      " • All existing pods of that node, can be moved to another node without any problem. If the above tw\n",
      " On the VKS system, you can turn on Auto Scaling when: Scale down mechanism: the Procuracy system\n",
      "pe\n",
      " • Minimum node : minimum number of nodes that the Cluster must have. • Maximum node : maximum numbe\n",
      " Default Max surge = 1 - upgrade only one node at a time. ◦ Max unavailable : limits the number of n\n",
      " • If there is a node with low utilization (availability) at < 50% and all pods of that node can be \n",
      "\f187 Upgrading Node Group Version\n",
      "Currently, our VKS system has supported you to upgrade Node Group \n",
      "vngcloud.vn/overview Step 2� At the Overview screen , select the Kubernetes Cluster menu. Select a C\n",
      " The new version needs to be valid and compatible with the current version of the cluster. Specifica\n",
      " Attention: • Upgrading Node Group Version is optional and independent of upgrading Control Plane Ve\n",
      "tech/overview\f188 upgrades the Node Group Version when the current K8S Version being used for your C\n",
      " • Stop running tasks: Stop running tasks on the cluster to avoid affecting the upgrade process. Whi\n",
      " • Test applications: Test applications running on the cluster to ensure they work properly after th\n",
      " You can assign key-value pairs to Kubernetes objects such as Pod, Node, Service, Deployment, etc. S\n",
      " • Lable can be used for a variety of purposes, including: ◦ Classify objects based on criteria such\n",
      "2 - This label indicates the object is related to version 1.7.2. To create a Lable for a Node Group,\n",
      "vngcloud.vn/overview Step 2� At the previously created Cluster, select Create a Node group. Lable Cr\n",
      "tech/overview\f190 Step 3� At the Node Group initialization screen, we have set up information for yo\n",
      " Alternatively, you can enter the key as a DNS subdomain, for example: example.com/my-app • Enter th\n",
      " Step 6� When the Node Group status is Active , you can view Node Group information by selecting Nod\n",
      "0 ...,disktype=ssd,kube\n",
      "worker1 Ready <none> 1d v1.13.\n",
      "0 ...,kubernetes.io/hos\n",
      "worker2 Ready <none> 1d v1.\n",
      "13.0 ...,kubernetes.\n",
      "io/hos Use Lable with nodeSelector http://example.com/my-app\f191 • Create a my-pod.yaml file contain\n",
      " Specifically: Specifically: • Each Taint includes: ◦ Key is a string of characters used to identify\n",
      " ▪ PreferNoSchedule: Kubernetes will try to prioritize not scheduling the Pod to the Node with this \n",
      " For example: • Relationship between Taint and Toleration: When Kubernetes schedules a Pod, Kubernet\n",
      "io/master:NoSchedule - prevents regular Pods from being run on this Node. To create a Taint for a No\n",
      " Step 3� At the Node Group initialization screen, we have set up information for your Node Group. Yo\n",
      " tolerations: - key: node.role.kubernetes.io/master effect: NoSchedule Create Taints https://vks.con\n",
      "vngcloud.tech/overview\f193 Alternatively, you can enter the key as a DNS subdomain, for example: exa\n",
      " Please wait a few minutes for us to initialize your Node Group. The status of the Node Group is cur\n",
      "role.kubernetes.io/master:NoSchedule. kubectl taint node my-master node.role.\n",
      "kubernetes.io/master:NoSchedule apiVersion: v1\n",
      "kind: Pod\n",
      "metadata: name: my-pod\n",
      "spec: tolerations: -\n",
      "com/my-app\f194 Network\f195 Working with Application\n",
      "Load Balancer (ALB) • Application Load Balancer \n",
      " For more information about ALB, please refer to �How it works �ALB�� Model: Overview What is ALB?\f1\n",
      " Unlike other Controller types that run as part of kube-controller-manager . VNGCloud Ingress Contro\n",
      " • Create or use a service account created on IAM and attach policy: vLBFullAccess , vServerFullAcce\n",
      " ◦ You need to change the security level for your Cluster or you need to open more ports for specifi\n",
      "vn/iam/service-accounts\f198 Attention: When you initialize the Cluster according to the instructions\n",
      " To create a service account, go here and follow these steps: ◦ Select \" Create a Service Account \",\n",
      "vn/iam/service-accounts\f199 • Install Helm version 3.0 or higher. Refer to https://helm.sh/docs/intr\n",
      " • Create nginx-service-lb7.yaml file with the following content: helm repo add vks-helm-charts http\n",
      "clientID= <Lấy ClientID của Service A --set cloudConfig.global.clientSecret= <Lấy ClientSecret của S\n",
      "sh/docs/intro/install/\f200 • Deploy This deployment equals: • Run the following command to test Depl\n",
      "If you do not have an Application Load Balancer previously created on the vLB system. Now, when crea\n",
      " At this point, you can create the nginx-ingress.yaml file as follows: kubectl get svc,deploy,pod -o\n",
      "96.25.133 <none> 80:32572 NAME READY UP-TO-DATE AVAILABLE AGE CONTA\n",
      "deployment.apps/nginx-app 1/1 1 \n",
      "vn/load-balancer-id\f202 • Run the following command to deploy Ingress Once you have deployed Ingress\n",
      "yaml Ingredient Quantity Properties ALB Package first VNG ALB_Small Listener 2 • 1 listener with HTT\n",
      "crt and tls.key, which are the certificate and private key to use for TLS. If you want to use a Cert\n",
      "If you already have a previously initialized Application Load Balancer on the vLB system and you wan\n",
      "k8s.io/v1\n",
      "kind: Ingress\n",
      "metadata: name: example-ingress annotations: # kubernetes.io/ingress.class: \n",
      "vn/certificate-ids: \"secret-a6d20ec6-f3e5-499a-981b-b1484\n",
      "spec: ingressClassName: \"vngcloud\" default\n",
      " If: ◦ Your ALB currently has 2 listeners in it: ▪ 1 listener has HTTP protocol configuration and po\n",
      "io/ingress.class: \"vngcloud\" # this annotation is deprec vks.vngcloud.vn/load-balancer-id: \"lb-2b9d8\n",
      "vn/certificate-ids: \"secret-a6d20ec6-f3e5-499a-981b-b1484\n",
      "spec: ingressClassName: \"vngcloud\" default\n",
      "vngcloud.vn/display/VKSVI/Ingress+for+an+Application+Load+Balancer\f206 • 1 listener has HTTP protoco\n",
      " • Or you can add/edit/delete policies in your ALB by editing the following parameters in the ingres\n",
      " ◦ ingressClassName : you need to specify this field value as \"vngcloud\" to use vngcloud-ingress-con\n",
      "vn/certificate-ids: \"secret-58542bfb-f410-4095-9e1c-34cd\n",
      "spec: ingressClassName: \"vngcloud\" defaultB\n",
      " ◦ spec: Ingress configuration, including traffic route rules according to the conditions of incomin\n",
      "181.129 80 103m kubectl describe ingress nginx-ingress Step 4: Check and edit the created Ingress re\n",
      "129/ Name: nginx-ingress\n",
      "Labels: <none>\n",
      "Namespace: default\n",
      "Address: 180.93.181.129\n",
      "Ingress Class: vn\n",
      "24.202:80)\n",
      "Rules: Host Path Backends ---- ---- ------- * /path1 nginx-service:80 (172.16.24.202:80)\n",
      "\n",
      "vngcloud.vn/load-balancer-id: lb-6cdea8fd-4589-410e-933\n",
      "Events: <none> kubectl edit ingress nginx-in\n",
      "129/\f210\f211 Configure for an\n",
      "Application Load Balancer\n",
      "On the �Ingress for an Application Load Bala\n",
      " After you have implemented Ingress deployment following the instructions at Ingress for an Applicat\n",
      " Now, when creating an Ingress, enter the Load Balancer ID information into this annotation. After y\n",
      " ◦ Your ALB does not have either or\f214 both listeners with the above configura tion, we will automa\n",
      "vn/load\n",
      "balancer-name Optional • Annotation vks.vngclou\n",
      "d.vn/loadbalancername will be used if you do\n",
      "vn/loadbalancername only makes sense when you create a new Ingress resource. After the Ingress resou\n",
      " This ALB will be displayed on vLB Portal, details can be accessed here • If you already have a prev\n",
      " • If you already have an ACTIVE vLB host and you want to integrate this host into your K8S cluster,\n",
      "vngcloud.vn/tags Optional • The tag is added to your ALB. vks.vngcloud.vn/scheme Optional • Default \n",
      " vks.vngcloud.vn/security\n",
      "groups Optional • By default, a default security group will be created acc\n",
      "vn/inbound\n",
      "cidrs Optional • Default All CIRD� 0.0.0.0/0 vks.vngcloud.\n",
      "vn/healthy\n",
      "threshold-count Optional • Default 3 vks.vngcloud.vn/unhealthy\n",
      "threshold-count Optional •\n",
      "vngcloud.vn/healthchec k-timeout-seconds Optional • Default 5 vks.vngcloud.vn/healthchec k-protocol \n",
      "vngcloud.vn/healthchec k-http-method Optional • Default GET . User can choose one of GET / POST / PU\n",
      "vngcloud.vn/healthchec k-http-version Optional • Default 1.0 . Users can choose one of the values 1.\n",
      "1 vks.vngcloud.vn/healthchec k-http-domain-name Optional • Default is empty vks.vngcloud.vn/healthch\n",
      "vngcloud.vn/success\n",
      "codes Optional • Default 200 vks.vngcloud.vn/idle\n",
      "timeout-client Optional • Defa\n",
      "vn/idle\n",
      "timeout-member Optional • Default 50 vks.vngcloud.vn/idle\n",
      "timeout-connection Optional • Defa\n",
      " The user can select one of the values ROUND_ROBI N / LEAST_CONN ECTIONS / SOURCE_IP\f219 The Ingress\n",
      "vn/enable\n",
      "sticky-session Optional • Default false . vks.vngcloud.vn/enable-tls\n",
      "encryption Optional •\n",
      "vn/target\n",
      "node-labels Optional • Default is empty vks.vngcloud.vn/certificate\n",
      "ids Optional • Default\n",
      "crt and tls.key, which are the certificate and private key to use for TLS. Specifically, you need to\n",
      " There are three supported pathTypes: • Exact: Matches the URL path with absolute precision and is c\n",
      " A URL request is considered to match a path field (configured in the Ingress specification) when th\n",
      " Exact /example1 /host1 Are not Exact /example1 /example1/ Are not Exact /example1/ /example1 Are no\n",
      " Both host and path must match the content of the incoming request before the load balancer directs \n",
      "com example2.example1.com Have *. example1.com baz.\n",
      "example2.example1.\n",
      "com Are not *. example1.com example1.\n",
      "com Are not Ingress rule http://example1.com/\f223 ALB Limitation A few notes about the limitations o\n",
      " For limits on the number of listeners, number of pools, number of policies, please refer to �Resour\n",
      " We will further upgrade this part in the next release versions. • Currently, vLB does not support t\n",
      " Note Limit https://docs-vngcloud-vn.translate.goog/vng-cloud-document/v/vn/vks/network/lam-viec-voi\n",
      " It is responsible for managing VNG Cloud resources for Kubernetes clusters, including: ◦ Create and\n",
      ". • Create a Kubernetes cluster on VNGCloud, or use an existing cluster. Note: make sure you have do\n",
      " ◦ After successful creation, you need to save the Client_ID and Secret_Key of the Service Account t\n",
      " If you have enabled the Enable vLB Native Integration Driver option , then we have pre-installed th\n",
      " • Uninstall cloud-controller-manager • Besides, you can delete the Service Account being used for t\n",
      " Refer to https://helm.sh/docs/intro/install/ for instructions on how to install. • Add this repo to\n",
      "global.clientID= <Lấy ClientID của Service A --set cloudConfig.global.clientSecret= <Lấy ClientSecre\n",
      "sh/docs/intro/install/\f228 1.If you do not have a previously initialized Network Load Balancer avail\n",
      "1 ports: - containerPort: 80\n",
      "--apiVersion: v1\n",
      "kind: Service\n",
      "metadata: name: nginx-service\n",
      "spec: sele\n",
      " At this point, please enter the Load Balancer ID information into the vks.vngcloud.vn/load-balancer\n",
      "vn/udp-server imagePullPolicy: Always ports: - containerPort: 10001 protocol: UDP\n",
      "--\n",
      "apiVersion: v1\n",
      "\n",
      "vn/package-id: \"lbp-ddbf9313-3f4c-471b-afd5-f6a3305159fc vks.vngcloud.vn/load-balancer-id: \"lb-2b9d8\n",
      " apiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata: name: http-apache2-deployment\n",
      "spec: replicas: 2 sele\n",
      " ◦ spec: Configure the conditions of incoming requests. For general information about working with v\n",
      "0.1 <none> 443/T\n",
      "service/nginx-service LoadBalancer 10.96.74.154 <pending> 80:31 NAME READY UP-TO-DA\n",
      "apps/nginx-app 0/1 1 0 2s nginx NAME READY STATUS RESTARTS A\n",
      "pod/nginx-app-7f45b65946-bmrcf 0/1 Cont\n",
      "93.181.20/ http://Endpoint/ Step 3: To access the just exported nginx app, you can\n",
      "use the URL with \n",
      "vn/vserver/load-balancer/vlb/detail/lb-927c0b5f-5bcf-4ee1-b645-41d6a0caeecb\f235\f236 Configure for a\n",
      "\n",
      " This NLB will be displayed on vLB Portal, details can be accessed here • If you already have a prev\n",
      "vn/vserver/load-balancer/vlb\f237 vks.vngcloud.vn/load\n",
      "balancer-name Optional • Annotation vks.vngclo\n",
      " • Annotation vks.vngcloud.vn/\n",
      "load-balancername only makes sense when you create a new load balance\n",
      " • When you use this annotation, if you do not already have a previously initialized Network Load Ba\n",
      "vn/vserver/load-balancer/vlb\f238 the vLB system and you want to reuse the NLB for your cluster. Now,\n",
      " • If you already have an ACTIVE vLB host and you want to integrate this host into your K8S cluster,\n",
      "vngcloud.vn/scheme Optional • Default is internet\n",
      "facing , you can change it to internal depending o\n",
      " vks.vngcloud.vn/inbound\n",
      "cidrs Optional • Default All CIRD� 0.0.0.\n",
      "0/0 vks.vngcloud.vn/healthy\n",
      "threshold-count Optional • Default 3 vks.vngcloud.vn/unhealth y-threshol\n",
      "vngcloud.vn/healthch eck-interval-seconds Optional • Default 30 vks.vngcloud.vn/healthch eck-timeout\n",
      "vn/healthch eck-protocol Optional • Default TCP . Users can select one of the values TCP/ HTTP/ HTTP\n",
      "vngcloud.vn/healthch eck-path Optional • Default / vks.vngcloud.vn/healthch eck-http-version Optiona\n",
      " Users can choose one of the values 1.0, 1.1 vks.vngcloud.vn/healthch eck-http-domain-name Optional \n",
      "vngcloud.vn/healthch eck-port Optional • Default traffic port vks.vngcloud.vn/success\n",
      "codes Optional\n",
      "vn/idle\n",
      "timeout-client Optional • Default 50 vks.vngcloud.vn/idle\n",
      "timeout-member Optional • Default \n",
      "vngcloud.vn/pool\n",
      "algorithm Optional • Default ROUND_ROBIN . The user can select one of the values RO\n",
      "vngcloud.vn/enable\n",
      "proxy-protocol Optional • Default is empty. The user specifies a list of service \n",
      " For limits on the number of listeners, number of pools, number of policies, please refer to Resourc\n",
      "vngcloud.vn/pages/viewpage.action?pageId=59802094\f242 CNI CNI �Container Network Interface) is a sta\n",
      " The CNI plugin performs the following tasks: • Assign IP address: Assign a unique IP address to the\n",
      " • Connecting between different VPCs : Use VPC Peering to connect nodes between different VPCs. • Co\n",
      " Compatible with many infrastructures but performance can be affected by tunnel overhead . • Cilium \n",
      " When to use Cilium VPC Native Routing : high performance requirements, easy connectivity to externa\n",
      " Model\f245 • When pods need to communicate with pods on other nodes, packets are encapsulated into o\n",
      " If you do not have any SSH key, please initialize SSH key following the instructions here . • kubec\n",
      "console.vngcloud.vn/overview Step 2� On the Overview screen , select Activate. Step 3� Wait until we\n",
      " Step 4� At the Cluster initialization screen, we have set up the information for the Cluster and a \n",
      "111.0.0/16 , corresponding to 65536 IPs Subnet A smaller IP address range belonging to the VPC. Each\n",
      " In the picture, we choose Subnet with Primary IP range of 10.111.0.0/24 , corresponding to 256 IPs \n",
      "16.0.0/16 . The pods will get IP from this IP range.\f247 Attention: • Only one networktype: In a clu\n",
      " This allows you to configure each node group in the cluster to be located on different subnets with\n",
      " Deploy a Workload\f248 Step 1� Access https://vks.console.vngcloud.vn/k8s-cluster Step 2� The Cluste\n",
      " Step 3 : Rename this file to config and save it to the ~/.kube/config folder Step 4� Perform Cluste\n",
      "8\n",
      "vks-cluster01-nodegroup01-22e98 Ready <none> 19h v1.28.8\n",
      "vks-cluster01-nodegroup01-36911 Ready <no\n",
      "28.8 k get pods -A https://vks.console-dev.vngcloud.tech/overview\f249 Step 2� Deploy nginx on the ne\n",
      "yaml file with the following content: NAMESPACE NAME READY STAT\n",
      "kube-system calico-kube-controllers-\n",
      "0/16 that we specified above: apiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata: name: nginx-app\n",
      "spec: s\n",
      "19\n",
      "nginx-app-7c79c4bf97-5hgwp 1/1 Running 0 49s 172.16.19\n",
      "nginx-app-7c79c4bf97-5l79h 1/1 Running 0 4\n",
      "16.22\n",
      "nginx-app-7c79c4bf97-5szc6 1/1 Running 0 49s 172.16.83\n",
      "nginx-app-7c79c4bf97-9272q 1/1 Running \n",
      "16\n",
      "nginx-app-7c79c4bf97-cgwrj 1/1 Running 0 49s 172.16.67\n",
      "nginx-app-7c79c4bf97-fhlg4 1/1 Running 0 4\n",
      "16.83\n",
      "nginx-app-7c79c4bf97-gh6hj 1/1 Running 0 49s 172.16.16\n",
      "nginx-app-7c79c4bf97-hx2rn 1/1 Running \n",
      "83\n",
      "nginx-app-7c79c4bf97-jv26j 1/1 Running 0 49s 172.16.16\n",
      "nginx-app-7c79c4bf97-km7p4 1/1 Running 0 4\n",
      "16.16\n",
      "nginx-app-7c79c4bf97-lvj6g 1/1 Running 0 49s 172.16.67\n",
      "nginx-app-7c79c4bf97-nhhdk 1/1 Running \n",
      "22\n",
      "nginx-app-7c79c4bf97-qr2lm 1/1 Running 0 49s 172.16.67\n",
      "nginx-app-7c79c4bf97-x4ztb 1/1 Running 0 4\n",
      "16.67 kubectl describe pod nginx-app-7c79c4bf97-2xbwd\f252 Using CNI Cilium Overlay\n",
      "Overview CNI Cili\n",
      " Model\f253 • lxc01/lxc02 � cilium_host : Packets from Pods are forwarded to cilium_host , which is t\n",
      " • There is at least 1 SSH key in ACTIVE state . If you do not have any SSH key, please initialize S\n",
      " To initialize a Cluster, follow the steps below: Step 1� Access https://vks.console.vngcloud.vn/ove\n",
      " After successfully Activating, select Create a Cluster. Necessary conditions Initialize a Cluster u\n",
      " In the picture, we choose VPC with IP range 10.111.0.0/16 , corresponding to 65536 IPs Subnet A sma\n",
      " The Subnet must be within the IP range of the selected VPC. In the picture, we choose Subnet with P\n",
      " CIDR The virtual network range that the pods will use In the picture, we choose the virtual network\n",
      "\f255 Attention: • Only one networktype: In a cluster, you can use only one of three networktypes: Ca\n",
      " Below are instructions for deploying an nginx deployment and testing IP assignment for the pods dep\n",
      " This file will give you full access to your Cluster. Step 3 : Rename this file to config and save i\n",
      "29.1\n",
      "vks-cluster-02-nodegroup-7fb09-430aa Ready <none> 5m52s v1.29.1 k get pods -A https://vks.conso\n",
      "vngcloud.tech/overview\f257 Step 2� Deploy nginx on the newly created cluster: • Initialize the nginx\n",
      "16.0.0/16 that we specified above: apiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata: name: nginx-app\n",
      "sp\n",
      "0.\n",
      "nginx-app-7c79c4bf97-669z9 1/1 Running 0 83s 172.16.1.\n",
      "nginx-app-7c79c4bf97-7hqp5 1/1 Running 0 8\n",
      "16.1.\n",
      "nginx-app-7c79c4bf97-8fjhm 1/1 Running 0 83s 172.16.2.\n",
      "\n",
      "nginx-app-7c79c4bf97-8xmfm 1/1 Running 0 83s 172.16.0.\n",
      "nginx-app-7c79c4bf97-9b4px 1/1 Running 0 83s\n",
      "2.\n",
      "nginx-app-7c79c4bf97-b7vlg 1/1 Running 0 83s 172.16.1.\n",
      "nginx-app-7c79c4bf97-bc6r4 1/1 Running 0 8\n",
      "16.0.\n",
      "nginx-app-7c79c4bf97-flkz5 1/1 Running 0 83s 172.16.2.\n",
      "\n",
      "nginx-app-7c79c4bf97-k55j6 1/1 Running 0 83s 172.16.2.\n",
      "nginx-app-7c79c4bf97-l9p8p 1/1 Running 0 83s\n",
      "2.\n",
      "nginx-app-7c79c4bf97-llnfq 1/1 Running 0 83s 172.16.1.\n",
      "nginx-app-7c79c4bf97-mg9t8 1/1 Running 0 8\n",
      "16.0.\n",
      "nginx-app-7c79c4bf97-mlh7g 1/1 Running 0 83s 172.16.2.\n",
      "\n",
      "nginx-app-7c79c4bf97-n946h 1/1 Running 0 83s 172.16.1.\n",
      "nginx-app-7c79c4bf97-p9k42 1/1 Running 0 83s\n",
      "0.\n",
      "nginx-app-7c79c4bf97-sl4b8 1/1 Running 0 83s 172.16.1.\n",
      "nginx-app-7c79c4bf97-tdtjc 1/1 Running 0 8\n",
      "16.2.\n",
      "nginx-app-7c79c4bf97-zwxps 1/1 Running 0 83s 172.16.2.\n",
      "\n",
      "nginx-app-7c79c4bf97-zxx87 1/1 Running 0 83s 172.16.0. kubectl describe pod nginx-app-7c79c4bf97-4l\n",
      " On VKS, CNI �Container Network Interface) Cilium VPC Native Routing operates according to the follo\n",
      " To be able to initialize a Cluster and Deploy a Workload , you need: • There is at least 1 VPC and \n",
      " ◦ Step 3: Here, if you don't have any VPC yet, please select Create VPC by entering the VPC name an\n",
      " For example, if you set Primary CIDR to 10.1.0.0/24, the IP addresses of the VMs will be in the ran\n",
      "0.1 to 10.1.0.254.\n",
      " ▪ Secondary CIDR : This is a secondary IP address range, used to provide additional IP addresses or\n",
      " The pods in each node use addresses from this CIDR and communicate over the virtual network. Attent\n",
      "0.0/24, then Secondary CIDR cannot be 10.1.0.0/20\f263 because it is within the range of Primary CIDR\n",
      " Instead, you can use a different address range like 10.1.16.0/20. • There is at least 1 SSH key in \n",
      " If you do not have any SSH key, please initialize SSH key following the instructions here . • kubec\n",
      " For example, when running an NGINX pod on a node, you must permit traffic on port 80 to ensure requ\n",
      "vn/overview Step 2� On the Overview screen , select Activate. Step 3� Wait until we successfully ini\n",
      "vngcloud.vn/vng-cloud-document/v/vn/vserver/compute-hcm03-1a/security/ssh-key-bo-khoa\f264 • Network \n",
      "0/16 , corresponding to 65536 IPs Subnet A smaller IP address range belonging to the VPC. Each node \n",
      "0.0/24 , corresponding to 256 IPs Default Pod IP range This is the secondary IP address range used f\n",
      "111.160.0/20 Corresponding to 4096 IPs for pods Node CIDR mask size CIDR size for nodes. This parame\n",
      " You can refer to the table below to understand how to calculate the number of IP addresses that can\n",
      "0/16 • Subnet: ◦ Primary IP Range: 10.111.0.0/24 ◦ Secondary IP Range: 10.111.\n",
      "160.0/20 • Node CIDR mask size: Selectable values range from /24 to /26 . Node CIDR mask size Number\n",
      " However, each Secondary IP Range can only be used by a single cluster. This helps avoid IP address \n",
      " At this time, you need to create a new node group with a secondary IP range that is not used on any\n",
      " Deploy a Workload\f268 Step 1� Access https://vks.console.vngcloud.vn/k8s-cluster Step 2� The Cluste\n",
      " Step 3 : Rename this file to config and save it to the ~/.kube/config folder Step 4� Perform Cluste\n",
      "yaml file with the following content: NAMESPACE NAME READY STATU\n",
      "kube-system cilium-envoy-2g22l 1/1 \n",
      " Specifically: • First, you need to install Cilium CLI following the instructions here . • After ins\n",
      "16\n",
      "nginx-app-7c79c4bf97-9tjw7 1/1 Running 0 31s 10.111.16\n",
      "nginx-app-7c79c4bf97-c6vx7 1/1 Running 0 3\n",
      "111.16\n",
      "nginx-app-7c79c4bf97-cggfq 1/1 Running 0 31s 10.111.16\n",
      "nginx-app-7c79c4bf97-cz4xc 1/1 Running\n",
      "16\n",
      "nginx-app-7c79c4bf97-d84rb 1/1 Running 0 31s 10.111.16\n",
      "nginx-app-7c79c4bf97-dbmt7 1/1 Running 0 3\n",
      "111.16\n",
      "nginx-app-7c79c4bf97-km7tx 1/1 Running 0 31s 10.111.16\n",
      "nginx-app-7c79c4bf97-lmk7c 1/1 Running\n",
      "16\n",
      "nginx-app-7c79c4bf97-mc24h 1/1 Running 0 31s 10.111.16\n",
      "nginx-app-7c79c4bf97-n4zvf 1/1 Running 0 3\n",
      "111.16\n",
      "nginx-app-7c79c4bf97-qtjjx 1/1 Running 0 31s 10.111.16\n",
      "nginx-app-7c79c4bf97-rp4bt 1/1 Running\n",
      "16\n",
      "nginx-app-7c79c4bf97-sk7tf 1/1 Running 0 31s 10.111.16\n",
      "nginx-app-7c79c4bf97-x8jxm 1/1 Running 0 3\n",
      "111.16 kubectl describe pod nginx-app-7c79c4bf97-6v88s cilium status wait https://docs.cilium.io/en/\n",
      "vn/81-vks-public/c cilium-envoy vcr.vngcloud.vn/81-vks-public/c hubble-relay vcr.vngcloud.vn/81-vks-\n",
      "vngcloud.vn/81-vks-public/c hubble-ui vcr.vngcloud.vn/81-vks-public/c cilium-operator vcr.vngcloud.\n",
      "vn/81-vks-public/c kubectl -n kube-system exec ds/cilium -- cilium-health status --probe\f273 Additio\n",
      "53 to a server in the same VPC with IP address: 10.111.0.10� • If the result is as follows, the conn\n",
      "0.8: ICMP to stack: OK, RTT=306.523µs HTTP to agent: OK, RTT=206.191µs Endpoint connectivity to 10.1\n",
      "160.91: ICMP to stack: OK, RTT=307.205µs HTTP to agent: OK, RTT=365.113µs vks-cluster-democilium-nod\n",
      "0.14: ICMP to stack: OK, RTT=1.90859ms HTTP to agent: OK, RTT=344.725µs Endpoint connectivity to 10.\n",
      "161.9: ICMP to stack: OK, RTT=1.889682ms HTTP to agent: OK, RTT=549.887µs vks-cluster-democilium-nod\n",
      "0.9: ICMP to stack: OK, RTT=1.920985ms HTTP to agent: OK, RTT=706.376µs Endpoint connectivity to 10.\n",
      "160.223: ICMP to stack: OK, RTT=1.919709ms HTTP to agent: OK, RTT=1.090877ms kubectl exec -it nginx-\n",
      "0.10 https://docs.cilium.io/en/stable/contributing/testing/e2e/\f274 PING 10.111.\n",
      "0.10 (10.111.0.10): 56 data bytes\n",
      "64 bytes from 10.\n",
      "111.0.10: seq=0 ttl=62 time=3.327 ms\n",
      "64 bytes from 10.111.\n",
      "0.10: seq=1 ttl=62 time=0.541 ms\n",
      "64 bytes from 10.111.0.\n",
      "10: seq=2 ttl=62 time=0.472 ms\n",
      "64 bytes from 10.111.0.10: seq=3 ttl=62 time=0.\n",
      "463 ms\n",
      "--- 10.111.0.10 ping statistics --4 packets transmitted, 4 packets received, 0% packet loss\n",
      "r\n",
      "200/3.327 ms\f275 Storage\f276 Working with Container\n",
      "Storage Interface (CSI) • Container Storage Inte\n",
      " Attention: • When you initialize the Cluster according to the instructions above, if you have not e\n",
      " If you have a need for ReadWriteMany, you may consider using the NFS CSI Driver, as it allows multi\n",
      " Install VNGCloud BlockStorage CSI Driver • Install Helm version 3.0 or higher. Refer to https://hel\n",
      "github.io/vks\n",
      "helm repo update helm install vngcloud-blockstorage-csi-driver vks-helm-chart --replac\n",
      "console.vngcloud.vn/iam/service-accounts\f279 • For example, in the image below you have successfully\n",
      "\f280 • Deploy This deployment equals: • Run the following command to test Deployment apiVersion: app\n",
      "yaml file with the following content: Copy NAME TYPE CLUSTER-IP EXTERNAL-IP P\n",
      "service/kubernetes Clu\n",
      "215.192 <none> 3\n",
      "service/nginx-service LoadBalancer 10.96.179.221 <pending> 8 NAME READY UP-TO-DATE \n",
      "apps/nginx-app 1/1 1 1 16s ngi NAME READY STATUS RESTARTS AGE IP\n",
      "pod/nginx-app-7f45b65946-t7d7k 1/1 \n",
      "vn # The VNG-CLOUD parameters: type: vtype-61c3fc5b-f4e9-45b4-8957-8aa7b6029018 # The volume typ\n",
      "all\n",
      " Follow these steps to enable the Snapshot service: Step 1� Visit https://hcm-3.console.vngcloud.vn/\n",
      "console.vngcloud.vn/vserver/block-store/snapshot/overview\f284 Step 2� Select Activate Snapshot Servi\n",
      " Refer to https://helm.sh/docs/intro/install/ for instructions on how to install. • Add this repo to\n",
      "sh/docs/intro/install/\f285 • Run the following command to deploy Volume Snapshot • After applying th\n",
      "vngcloud.vn\n",
      "deletionPolicy: Delete\n",
      "parameters: force-create: \"false\"\n",
      "--\n",
      "apiVersion: snapshot.storage\n",
      "yaml kubectl get sc,pvc,pod -owide Create a snapshot.yaml file with the following content Check the \n",
      " For example, below I am changing the Persistent Volume IOPS from 200 �Volume type id = vtype-61c3fc\n",
      "vngc\n",
      "storageclass.storage.k8s.io/sc-iops-200-retain (default) bs.csi.\n",
      "vngc NAME STATUS VOLUME persistentvolumeclaim/my-expansion-pvc Bound pvc-14456f4a-ee9e-43 NAME READY\n",
      " To change the Disk Volume of the newly created Persistent Volume, run the following command: apiVer\n",
      "io/last-applied-configuration: | {\"apiVersion\":\"v1\",\"kind\":\"PersistentVolumeClaim\",\"metadata\":{\"a pv\n",
      "beta.kubernetes.io/storage-provisioner: bs.csi.vngcloud.\n",
      "vn volume.kubernetes.io/storage-provisioner: bs.csi.vngcloud.\n",
      "vn creationTimestamp: \"2024-04-21T14:16:53Z\" finalizers: - kubernetes.io/pvc-protection name: my-exp\n",
      "storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 20Gi Restore Persistent Vo\n",
      " Specifically, when you initialize a Cluser, we will automatically create several Security Groups wi\n",
      " Security group rule used for The default security group is automatically created for all Clusters\f2\n",
      "0.0.0/0 Default rule of all Security groups ANY IPv6 0�65535 ::/0 Default rule of all Security group\n",
      "0.0.0/0 Default rule of all Security groups ANY IPv6 0�65535 ::/0 Default rule of all Security group\n",
      "0.0/0 Default rule of all Security groups ANY IPv6 0�65535 ::/0 Default rule of all Security groups \n",
      " The purpose of this is often to upgrade the system, increase availability and fault tolerance, or t\n",
      ". Now, before starting to migrate, you need to proactively migrate these resources yourself. • Step \n",
      " Velero will perform queries, package the data, and upload them to an S3 Compatible Object Storage. \n",
      "translate.goog/vng-cloud-document/v/vn/vks/clusters\f296 Below are detailed instructions for common c\n",
      " • Perform download helper bash script and grand execute permission for this file ( velero_helper.sh\n",
      "sh\f298 apiVersion: v1\n",
      "kind: Service\n",
      "metadata: name: nginx namespace: mynamespace labels: app: nginx\n",
      "\n",
      ". • Node labels and node taints are the same as the old cluster. • Corresponding or alternative Stor\n",
      " Now, before starting to migrate, you need to migrate these resources yourself. For example, if you \n",
      " After the migration is complete, remember to reconfigure the database for your applications on VKS \n",
      " • Create an S3 key corresponding to this vStorage Project according to the instructions here . For \n",
      " • Create file credentials-velero with the following content: Install Velero on both source and dest\n",
      "1\n",
      "tar -xvf velero-v1.13.2-linux-amd64.tar.gz\n",
      "cp velero-v1.\n",
      "13.2-linux-amd64/velero /usr/local/bin velero install --provider aws \\ --plugins velero/velero-plugi\n",
      "github.io/vks-helm-char\n",
      "helm repo update\n",
      "helm install vngcloud-snapshot-controller vks-helm-charts/v\n",
      "/velero_helper.sh mark_volume -c ./velero_helper.sh mark_exclude -c apiVersion: snapshot.storage.\n",
      "k8s.io/v1\n",
      "kind: VolumeSnapshotClass\n",
      "metadata: name: vngcloud-vsclass labels: velero.io/csi-volumesna\n",
      "vn\n",
      "deletionPolicy: Delete # user can choose the VolumeSnapshotClass by setting annotation velero\n",
      "# u\n",
      " Step 3� If the vContainer version is supported by vKS �1.27, 1.28 and 1.29�, then you need to check\n",
      " • Ingress resources : Ingress resources managed by container-ingress-nginx\n",
      "controller will not work\n",
      " 2. Important Note: 1. Mapping StorageClass on vKS cluster 2. Use PersistentVolume as hostPath\f306 F\n",
      "io/v1 kind: StorageClass metadata: name: manual provisioner: kubernetes.io/no-provisioner volumeBind\n",
      ") and repeat Case 1, then mount the PVC into the Pod as usual: - matchExpressions: - key: kubernetes\n",
      "14.1-linux-amd64.tar.gz cp velero-v1.14.\n",
      "1 -linux-amd64/velero /usr/local/bin 3. Detailed implementation steps Step 1: Install Velero on both\n",
      ". then it is necessary to mark labels for all those resources. velero install \\ --provider aws \\ --p\n",
      "/credentials-velero \\ --bucket __________\\ # <= Adjust here --backup-location-config region=hcm03,s3\n",
      "io/exclude-from-backup=true cho từng resources ## Với Cinder CSI kubectl -n kube-system label Statef\n",
      "csi.openstack.org , need to perform StorageClass mapping between 2 clusters vContainer and vKS ◦ Map\n",
      "yaml file and apply on VKS cluster apiVersion: v1 kind: ConfigMap metadata: name: change-storage-cla\n",
      "io/pod-volume-restore: RestoreItemAction data: secCtx: | capabilities: drop: [] add: [] allowPrivile\n",
      "com/vngcloud/velero/main/velero_helper.sh\f317 apiVersion: v1\n",
      "kind: Service\n",
      "metadata: name: nginx nam\n",
      ".. • Node labels and node taints are the same as the old cluster. • Corresponding or alternative Sto\n",
      " For example, you may have private resources such as images, databases, etc. Now, before starting to\n",
      " • Migrate Databases: you can use Relational Database Service �RDS� and Object Storage Service �OBS�\n",
      " • Create a vStorage Project, Container to receive the cluster's backup data according to instructio\n",
      "vn . • Create file credentials-velero with the following content: Install Velero on both source and \n",
      " You can run the command below to annotate backup of all volumes. • Additionally, you can mark not t\n",
      "2-linux-amd64.tar.gz\n",
      "cp velero-v1.13.2-linux-amd64/velero /usr/local/bin velero install \\ --provider\n",
      "9.0 \\ --use-node-agent \\ --use-volume-snapshots=false \\ --secret-file ./credentials-velero \\ --bucke\n",
      "/velero_helper.sh mark_exclude -c For Clusters on Amazon Elastic Kubernetes Service\n",
      "(EKS) At the sou\n",
      " velero restore create --item-operation-timeout 1m --from-backup eks-na velero restore create --item\n",
      " However, Velero only needs to deploy the daemonset on the node with the PV mount. The solution to t\n",
      "io/change-storage-class: RestoreItemAction\n",
      "data: _______old_storage_class_______: _______new_storage\n",
      "velero.io/backup-volumes=volume1,volume2 • Or you can automatically find volumes by: ./helper.sh che\n",
      "sh mark_volume Mark the Volumes you want to backup and\n",
      "unnecessary resources 1. Convert hostPath Vol\n",
      "com/vngcloud/velero/main/velero_helper.sh\f325 Because VKS operates under the Fully Managed Control P\n",
      ".. will also be ignored. • Identify resources that do not need backup via the command: When performi\n",
      " • Check lable and taint via command: • If your Storage Class is different between the source Cluste\n",
      "/helper.sh check_node_label ./helper.sh check_node_taint @ kubectl get sc\n",
      "NAME PROVISIONER RECLAIMPO\n",
      "vn Retain I\n",
      "sc-ssd-10000-delete (default) csi.vngcloud.vn Delete I 4. Check label and taint of node \n",
      "io/plugin-config: \"\" velero.io/change-storage-class: RestoreItemAction\n",
      "data: sc-iops-200-retain: ssd\n",
      " Terraform requires a cloud provider account and key to be configured along with a Terraform configu\n",
      " Specifically, at the IAM site, you can: • Select \" Create a Service Account \", enter a name for the\n",
      "vn/\f328 • After successful creation, you need to save the Client_ID and Secret_Key of the Service Ac\n",
      " Install Terraform: • Download and install Terraform for your operating system from https://develope\n",
      "tf and declare Service Account information in this file. • Create a file main.tf and define the Kube\n",
      " • On the main.tf file , you need to be able to add resources to create a Cluster/ Node Group: ◦ Cre\n",
      " This allows you to add or remove Node Groups without recreating the entire Cluster. If you declare \n",
      "0.0/16\" vpc_id = \"net-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\" subnet_id = \"sub-xxxxxxxx-xxxx-xxxxx-xx\n",
      "0.0/16\" vpc_id = \"net-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\" subnet_id = \"sub-xxxxxxxx-xxxx-xxxxx-xx\n",
      "2.2\" } }\n",
      "} provider \"vngcloud\" { token_url = \"https://iamapis.vngcloud.vn/accounts-api/v2/auth/to cl\n",
      "client_secret vserver_base_url = \"https://hcm-3.api.vngcloud.vn/vserver/vserver-gatew vlb_base_url =\n",
      "vngcloud.vn/vserver/vlb-gateway\"\n",
      "} resource \"vngcloud_vks_cluster\" \"primary\" { name = \"cluster-demo\"\n",
      "0.0/16\" enable_private_cluster = false network_type = \"CALICO\" vpc_id = \"net-70ef12d4-d619-43fc-88f0\n",
      "29.1 • Mode: Public Cluster and Private Node Group • Node Group name: my-nodegroup • Initial Node: 3\n",
      "vn/accounts-api/v2/auth/to client_id = var.client_id client_secret = var.client_secret vserver_base_\n",
      "vn/vserver/vserver-gatew vlb_base_url = \"https://hcm-3.api.vngcloud.vn/vserver/vlb-gateway\"\n",
      "} resour\n",
      "1\" cidr = \"172.16.0.0/16\" enable_private_cluster = false network_type = \"CALICO\" vpc_id = \"net-xxxxx\n",
      "id name= \"my-nodegroup\" num_nodes = 3 auto_scale_config { min_size = 0 max_size = 5 } upgrade_config\n",
      "vn/accounts-api/v2/auth/to client_id = var.client_id client_secret = var.client_secret vserver_base_\n",
      "vn/vserver/vserver-gatew vlb_base_url = \"https://hcm-3.api.vngcloud.vn/vserver/vlb-gateway\"\n",
      "} resour\n",
      "1\" cidr = \"172.16.0.0/16\" white_list_node_cidr = \"172.25.\n",
      "32.1/16\" enable_private_cluster = false network_type = \"CALICO\" vpc_id = \"net-xxxxxxxx-xxxx-xxxxx-xx\n",
      " • After completing the above information, run the command below: • Then, to see the changes that wi\n",
      "terraform.io/providers/vngcloud/vngcloud/latest/docs/resources/vks_cluster\f338 Monitoring\f339 Metric\n",
      " If you do not have it, you need to buy the Metric Quota here . 2. Create a Service Account and atta\n",
      " You need to install Helm on a server with kubeconfig containing enough permissions to interact with\n",
      " 2. Proceed to install Helm • Execute the following commands: • Check that Helm has been installed s\n",
      "asc | gpg --dearmor | sudo tee /us\n",
      "sudo apt-get install apt-transport-https --yes echo \"deb [arch=$(\n",
      "...) • Daemonset Agent: collect metrics for each k8s cluster node �CPU, Memory usage, ..\n",
      ".) 1. Add Helm vMonitor Platform Repo 2. Install charts • Check and delete related resources before \n",
      "io/helm-charts-vm\n",
      "helm repo update # Get Clusterrole vmonitor metric agent\n",
      "kubectl get clusterrole |\n",
      "iamClientID=<YOUR_CLIENT_ID_XXXXXXXXXXXXXXXXXXX> \\ --set vmonitor.iamClientSecret=<YOUR_CLIENT_SECRE\n",
      " You only need to pay for other resources that you actually use, including: • All nodes present in t\n",
      " Details on how to calculate Volume prices can be found here . Attention: • To ensure your Cluster o\n",
      "goog/vng-cloud-document/v/vn/vserver/compute-hcm03-1a/cach-tinh-gia-vserver\f344 Reference\f345 Kubern\n",
      "md#v1291 Version 1.28.8 2024�10�28 https://github.com/kubernetes/kubernetes/ blob/master/CHANGELOG/C\n",
      "md#v1288 Version 1.27.12 2024�06�28 https://github.com/kubernetes/kubernetes/ blob/master/CHANGELOG/\n",
      "md#v12712 https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.29.md#v1291\f346\n",
      "s-general-2�4 2\f349 eci.ins.s-general-4�8 4 eci.ins.s-general-8�16 8 eci.\n",
      "ins.s-general-16�32 16 s-general-2�4 2 s-general-4�8 4\f350 s-general-8�16 8 s-general-16�32 16 s-gen\n",
      "s1-standard-4�16 4 eci.ins.s1-standard-8�32 8 eci.ins.s1-standard-16�64 16 eci.\n",
      "ins.s1-standard-32�128 32 s1-highcpu-16�16 16\f356 s1-highcpux2�32�64 32 eci.ins.s1-highmem-2�16 2 ec\n",
      "s1-highmem-4�32 4 eci.ins.s1-highmem-8�64 8 eci.ins.s1-highmem-16�128 16 eci.\n",
      "ins.s1-highmem-32�256 32\f357 • GPU Code G eci.ins.s1-highcpu-4�4 4 eci.ins.\n",
      "s1-highcpu-8�8 8 eci.ins.s1-highcpu-16�16 16 Flavor Name CPU Memory g1-standard-4�16�1rtx2080ti 4 16\n",
      "kube_v1� 27�12 img-36ee0a61�863d-4d40-a768� 9b41269b8a62 v1.28.8 1_Ubuntu-22.kube_v1� 28�8 img-983d5\n",
      "1 1_Ubuntu-22.kube_v1� 29�1 img-108b3a77-ab58�4000�9b3e\n",
      "190d0b4b07fc\n"
     ]
    }
   ],
   "source": [
    "for doc in document_store.filter_documents():\n",
    "    print(doc.content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
